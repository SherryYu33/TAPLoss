{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fIKntoMivQqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e895025-d70c-440b-9dec-af29251bd181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hamming_lld_estimat 100%[===================>]  78.24M  9.22MB/s    in 8.5s    \n",
            "clean_fileid_0.wav. 100%[===================>] 312.54K  --.-KB/s    in 0.05s   \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PREFIX\"] = \"wget -q https://cmu.box.com/shared/static\"\n",
        "os.environ[\"SUFFIX\"] = \"--content-disposition --show-progress\"\n",
        "\n",
        "### Download acoustic estimator checkpoint\n",
        "!${PREFIX}/u5ejvjofkg5aqfagzp2bbuex6vcnk2as ${SUFFIX}\n",
        "\n",
        "### Download example clean speech waveform\n",
        "!${PREFIX}/2curyswb9gkuvikyazj5cj404vbb54dm ${SUFFIX}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import typing\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "global DEVICE \n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "hYFy62R4TuDC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global DEFAULT_SAMPLE_RATE\n",
        "global DEFAULT_N_FFT\n",
        "global DEFAULT_HOP_LENGTH\n",
        "global DEFAULT_WIN_LENGTH\n",
        "global DEFAULT_WINDOW\n",
        "\n",
        "### Waveform Threshold Limits\n",
        "MINIMUM_DURATION_IN_SECONDS = 5\n",
        "\n",
        "### Waveform Default Parameters\n",
        "DEFAULT_SAMPLE_RATE = 16000\n",
        "\n",
        "### Spectrogram Default Parameters\n",
        "DEFAULT_N_FFT       = 640\n",
        "DEFAULT_HOP_LENGTH  = 160\n",
        "DEFAULT_WIN_LENGTH  = 640\n",
        "DEFAULT_WINDOW      = torch.hamming_window(DEFAULT_WIN_LENGTH)"
      ],
      "metadata": {
        "id": "ob3jTWXtbanC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global ACOUSTIC_FEATURE_NAMES \n",
        "global ACOUSTIC_MEAN\n",
        "global ACOUSTIC_STANDARD_DEVIATION\n",
        "\n",
        "ACOUSTIC_FEATURE_NAMES = [\n",
        "    'Loudness_sma3',\n",
        "    'alphaRatio_sma3',\n",
        "    'hammarbergIndex_sma3',\n",
        "    'slope0-500_sma3',\n",
        "    'slope500-1500_sma3',\n",
        "    'spectralFlux_sma3',\n",
        "    'mfcc1_sma3',\n",
        "    'mfcc2_sma3',\n",
        "    'mfcc3_sma3',\n",
        "    'mfcc4_sma3',\n",
        "    'F0semitoneFrom27.5Hz_sma3nz',\n",
        "    'jitterLocal_sma3nz',\n",
        "    'shimmerLocaldB_sma3nz',\n",
        "    'HNRdBACF_sma3nz',\n",
        "    'logRelF0-H1-H2_sma3nz',\n",
        "    'logRelF0-H1-A3_sma3nz',\n",
        "    'F1frequency_sma3nz',\n",
        "    'F1bandwidth_sma3nz',\n",
        "    'F1amplitudeLogRelF0_sma3nz',\n",
        "    'F2frequency_sma3nz',\n",
        "    'F2bandwidth_sma3nz',\n",
        "    'F2amplitudeLogRelF0_sma3nz',\n",
        "    'F3frequency_sma3nz',\n",
        "    'F3bandwidth_sma3nz',\n",
        "    'F3amplitudeLogRelF0_sma3nz']\n",
        "\n",
        "ACOUSTIC_MEAN = torch.tensor(\n",
        "    [  2.31615782e-01, -5.02114248e+00,  7.16793156e+00,  1.40047576e-02,\n",
        "      -1.44424592e-03,  1.18291244e-01,  7.16937304e+00,  5.01161051e+00,\n",
        "       7.38044071e+00,  1.30544746e+00,  7.16783571e+00,  7.72617990e-03,\n",
        "       3.78611624e-01,  1.80594587e+00,  2.74223471e+00,  7.16790104e+00,\n",
        "       2.29371735e+02,  2.61031281e+02, -2.86713428e+01,  4.58741486e+02,\n",
        "       2.72984955e+02, -2.86713428e+01,  4.58874390e+02,  2.71175812e+02,\n",
        "      -2.86713428e+01], dtype=torch.float32)\n",
        "\n",
        "ACOUSTIC_STANDARD_DEVIATION = torch.tensor(\n",
        "    [ 4.24716711e-01, 1.09750290e+01, 1.51086359e+01, 2.98775751e-02,\n",
        "      1.85245797e-02, 2.39421308e-01, 1.63376312e+01, 1.22261524e+01,\n",
        "      1.53735695e+01, 1.42613926e+01, 1.21981163e+01, 2.58955006e-02,\n",
        "      8.05543840e-01, 3.83967781e+00, 6.79308844e+00, 1.41308403e+01,\n",
        "      3.49271667e+02, 6.28384338e+02, 6.05799637e+01, 6.89079407e+02,\n",
        "      5.62089905e+02, 6.05799637e+01, 1.09140088e+03, 5.42341919e+02,\n",
        "      6.05799637e+01], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "HOTDkJ65spI0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AcousticEstimator(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, DEVICE):\n",
        "        \n",
        "        super(AcousticEstimator, self).__init__()\n",
        "\n",
        "        self.DEVICE = DEVICE\n",
        "        \n",
        "        self.lstm = torch.nn.LSTM(642, 256, 4, bidirectional=True, batch_first=True)\n",
        "        \n",
        "        self.linear1 = torch.nn.Linear(512, 256)\n",
        "        self.linear2 = torch.nn.Linear(256, 128)\n",
        "        self.linear3 = torch.nn.Linear(128, 25)\n",
        "        \n",
        "        self.act = torch.nn.GELU()\n",
        "        \n",
        "    def forward(self, spectrogram):\n",
        "\n",
        "        hidden, _   = self.lstm(spectrogram)\n",
        "        hidden      = self.linear1(hidden)\n",
        "        hidden      = self.act(hidden)\n",
        "        hidden      = self.linear2(hidden)\n",
        "        hidden      = self.act(hidden)\n",
        "        acoustics   = self.linear3(hidden)\n",
        "        \n",
        "        return acoustics\n",
        "\n",
        "MODEL_PATH = \"./hamming_lld_estimator_13mse_13mae.pt\"\n",
        "\n",
        "CHECKPOINT = torch.load(MODEL_PATH, map_location=DEVICE)['model_state_dict']\n",
        "\n",
        "global ACOUSTIC_ESTIMATOR\n",
        "\n",
        "ACOUSTIC_ESTIMATOR = AcousticEstimator(DEVICE)\n",
        "ACOUSTIC_ESTIMATOR.load_state_dict(CHECKPOINT)\n",
        "ACOUSTIC_ESTIMATOR.to(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkWLRNzzR28z",
        "outputId": "05478eb9-7127-4ed6-d60a-aa414ad977da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AcousticEstimator(\n",
              "  (lstm): LSTM(642, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
              "  (linear1): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (linear3): Linear(in_features=128, out_features=25, bias=True)\n",
              "  (act): GELU(approximate='none')\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_waveformFromAudioPath(\n",
        "    audioPath : str) -> typing.Tuple[torch.FloatTensor, int]:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        audioPath (str): \n",
        "            Path to audio file.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        waveform (torch.FloatTensor): \n",
        "            A 1-D time sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        waveform, sampleRate = torchaudio.load(audioPath)\n",
        "\n",
        "        # Resamples the waveform if necessary at the new frequency using  \n",
        "        # bandlimited interpolation. [Smith, 2020].\n",
        "        waveform = torchaudio.functional.resample(\n",
        "            waveform  = waveform            , \n",
        "            orig_freq = sampleRate          , \n",
        "            new_freq  = DEFAULT_SAMPLE_RATE )\n",
        "        \n",
        "        return waveform[0], DEFAULT_SAMPLE_RATE\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        raise RuntimeError(\n",
        "            'Exception thrown in get_waveformFromAudioPath:\\n {}'.format(e))\n",
        "\n",
        "def get_spectrogramFromWaveform(\n",
        "    waveform   : torch.FloatTensor ,\n",
        "    sampleRate : int               ) -> torch.FloatTensor:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        waveform (torch.FloatTensor): \n",
        "            A 1-D time sequence.\n",
        "\n",
        "    Returns:\n",
        "    \n",
        "        spectrogram (torch.FloatTensor): \n",
        "            Returns a wrapped complex tensor of size [2*T, 2*(N_FFT+1)], where \n",
        "            N_FFT is the number of frequencies where STFT is applied and \n",
        "            T is the total number of frames used. \n",
        "            Note: Real and imaginary alternate in sequence.\n",
        "    \"\"\"\n",
        "    try:\n",
        "\n",
        "        # Check if waveform is not a 1-D time sequence.\n",
        "        if waveform.dim() != 1: \n",
        "\n",
        "            raise RuntimeError(\n",
        "                'RuntimeError in getSpectrogramFromWaveform: ' +\n",
        "                'waveform.dim() should be 1 but received {}'.format(\n",
        "                    waveform.dim()))\n",
        "\n",
        "        # Resamples the waveform if necessary at the new frequency using  \n",
        "        # bandlimited interpolation. [Smith, 2020].\n",
        "        waveform = torchaudio.functional.resample(\n",
        "            waveform  = waveform            , \n",
        "            orig_freq = sampleRate          , \n",
        "            new_freq  = DEFAULT_SAMPLE_RATE )\n",
        "        \n",
        "        # Check if waveform duration is less than minimum limit.\n",
        "        if len(waveform) < MINIMUM_DURATION_IN_SECONDS * DEFAULT_SAMPLE_RATE:\n",
        "            \n",
        "            raise RuntimeWarning(\n",
        "                'RuntimeWarning in getSpectrogramFromWaveform: ' +\n",
        "                'len(waveform) should be less than {} but received {}'.format(\n",
        "                    MINIMUM_DURATION_IN_SECONDS * DEFAULT_SAMPLE_RATE,\n",
        "                    len(waveform) ) )\n",
        "            \n",
        "        # See: https://pytorch.org/docs/stable/generated/torch.stft.html\n",
        "        spectrogram = torch.stft(\n",
        "            input          = waveform           , \n",
        "            n_fft          = DEFAULT_N_FFT      , \n",
        "            hop_length     = DEFAULT_HOP_LENGTH , \n",
        "            win_length     = DEFAULT_WIN_LENGTH , \n",
        "            window         = DEFAULT_WINDOW     ,\n",
        "            return_complex = False              )\n",
        "        \n",
        "        # Permute to make time first: (N_FFT//2+1, T, 2) -> (T, N_FFT//2+1, T)\n",
        "        spectrogram = spectrogram.permute(1, 0, 2)\n",
        "\n",
        "        # Alternate between corresponding real and imag components over time    \n",
        "        spectrogram = spectrogram.reshape(-1, DEFAULT_N_FFT + 2)\n",
        "\n",
        "        # Remove last 5 frames, whose targets were not available during training\n",
        "        spectrogram = spectrogram[:-5].float()\n",
        "        \n",
        "        return spectrogram\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        raise RuntimeError(\n",
        "            'Exception thrown in get_spectrogramFromWaveform: {}'.format(e))\n",
        "\n",
        "\n",
        "def get_spectrogramFromAudioPath(\n",
        "    audioPath : str) -> torch.FloatTensor:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        audioPath (str): \n",
        "            Path to audio file.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        spectrogram (torch.FloatTensor): \n",
        "            Returns a wrapped complex tensor of size [2*T, 2*(N_FFT+1)], where \n",
        "            N_FFT is the number of frequencies where STFT is applied and \n",
        "            T is the total number of frames used. \n",
        "            Note: Real and imaginary alternate in sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        waveform, sampleRate = get_waveformFromAudioPath(audioPath)\n",
        "\n",
        "        spectrogram = get_spectrogramFromWaveform(waveform, sampleRate)\n",
        "\n",
        "        return spectrogram\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        print('Exception thrown in get_spectrogramFromAudioPath: {}'.format(e))"
      ],
      "metadata": {
        "id": "PupuZSDAwCG6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acousticsFromSpectrogram(\n",
        "    estimator   : torch.nn.Module   ,\n",
        "    spectrogram : torch.FloatTensor ) -> torch.FloatTensor:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        estimator (torch.nn.Module):\n",
        "            See our ICASSP paper: [Yunyang, et al. 2023].\n",
        "\n",
        "        spectrogram (torch.FloatTensor): \n",
        "            A wrapped complex tensor of size [2*T, 2*(N_FFT+1)], where \n",
        "            N_FFT is the number of frequencies where STFT is applied and \n",
        "            T is the total number of frames used. \n",
        "            Note: Real and imaginary alternate in sequence.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        acoustics (torch.FloatTensor): \n",
        "            25 time series for each audio. Each time series represents an \n",
        "            acoustic. Our ICASSP paper: [Yunyang, et al. 2023].\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Check if spectrogram is not a 2-D time sequence.\n",
        "        if spectrogram.dim() != 2:\n",
        "\n",
        "            raise RuntimeError(\n",
        "                'RuntimeError in get_acousticsFromSpectrogram:\\n ' +\n",
        "                'waveform.dim() should be 2 but received {}'.format(\n",
        "                    spectrogram.dim()))\n",
        "            \n",
        "        # Check default frequency requirements are met\n",
        "        if spectrogram.shape[1] != 642:\n",
        "            raise RuntimeError(\n",
        "                'RuntimeError in get_acousticsFromSpectrogram:\\n ' +\n",
        "                'spectrogram.shape[1] should be 642 but received {}'.format(\n",
        "                    spectrogram.shape[1]))\n",
        "\n",
        "        # Check default duration requirement is met\n",
        "        if spectrogram.shape[0] < 125: \n",
        "            raise RuntimeError(\n",
        "                'RuntimeError in get_acousticsFromSpectrogram:\\n ' +\n",
        "                'spectrogram.shape[0] should be 125 but received {}'.format(\n",
        "                    spectrogram.shape[0]))\n",
        "\n",
        "        # Calculate acoustics using our estimator. [Yunyang, et al. 2023]\n",
        "        acoustics = estimator(spectrogram)\n",
        "\n",
        "        return acoustics\n",
        "\n",
        "    except Exception as e:\n",
        "        \n",
        "        raise RuntimeError(\n",
        "            'Exception thrown in get_acousticsFromSpectrogram:\\n {}'.format(e))\n",
        "        \n",
        "\n",
        "def get_acousticsFromWaveform(\n",
        "    estimator  : torch.nn.Module   ,\n",
        "    waveform   : torch.FloatTensor ,\n",
        "    sampleRate : int               ) -> torch.FloatTensor:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        estimator (torch.nn.Module): \n",
        "            See our ICASSP paper: [Yunyang, et al. 2023].\n",
        "\n",
        "        waveform (torch.FloatTensor): \n",
        "            A 1-D time sequence.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        acoustics (torch.FloatTensor): \n",
        "            25 time series for each audio. Each time series represents an \n",
        "            acoustic. \n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        spectrogram = get_spectrogramFromWaveform(waveform, sampleRate)\n",
        "\n",
        "        acoustics = get_acousticsFromSpectrogram(estimator, spectrogram)\n",
        "\n",
        "        return acoustics\n",
        "        \n",
        "    except Exception as e:\n",
        "\n",
        "        raise RuntimeError(\n",
        "            'Exception thrown in get_acousticsFromWaveform:\\n {}'.format(e))\n",
        "\n",
        "def get_acousticsFromAudioPath(\n",
        "    estimator : torch.nn.Module ,\n",
        "    audioPath : str             ) -> torch.FloatTensor:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        estimator (torch.nn.Module): \n",
        "            See our ICASSP paper: [Yunyang, et al. 2023].\n",
        "        \n",
        "        audioPath (str): \n",
        "            Path to audio file.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        acoustics (torch.FloatTensor): \n",
        "            25 time series for each audio. Each time series represents an \n",
        "            acoustic. Our ICASSP paper: [Yunyang, et al. 2023].\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        waveform, sampleRate = get_waveformFromAudioPath(audioPath)\n",
        "\n",
        "        acoustics = get_acousticsFromWaveform(estimator, waveform, sampleRate)\n",
        "\n",
        "        return acoustics\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        raise RuntimeError(\n",
        "            'Exception thrown in get_acousticsFromAudioPath:\\n {}'.format(e))"
      ],
      "metadata": {
        "id": "To2jgw4MxcEB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acousticsFromAudioPathList(\n",
        "    estimator     : torch.nn.Module  ,\n",
        "    audioPathList : typing.List[str] ) -> typing.List[torch.FloatTensor]:\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "\n",
        "        estimator (torch.nn.Module): \n",
        "            See our ICASSP paper: [Yunyang, et al. 2023].\n",
        "        \n",
        "        audioPath (str): \n",
        "            List of paths, each to an audio file.\n",
        "\n",
        "    Returns:\n",
        "\n",
        "        acoustics (list[torch.FloatTensor]): \n",
        "            A list of 25 time series for each audio. Each time series \n",
        "            represents an acoustic. Our ICASSP paper: [Yunyang, et al. 2023].\n",
        "    \"\"\"\n",
        "\n",
        "    acousticList = []\n",
        "\n",
        "    for audioPath in tqdm(audioPathList):\n",
        "\n",
        "        try:\n",
        "\n",
        "            acoustics = get_acousticsFromAudioPath(estimator, audioPath)\n",
        "\n",
        "            acousticList.append(acoustics)\n",
        "\n",
        "        except Exception as e:\n",
        "\n",
        "            print('Warning in get_acousticsFromAudioPathList:\\n {}'.format(e))\n",
        "\n",
        "            continue\n",
        "\n",
        "    return acousticList"
      ],
      "metadata": {
        "id": "PUU3j0q2R4-v"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage:\n",
        "#\n",
        "# get_acousticsFromAudioPathList(\n",
        "#     estimator     = ACOUSTIC_ESTIMATOR,\n",
        "#     audioPathList = [\"/content/clean_fileid_0.wav\", \"/content/clean_fileid_0.wav\"])"
      ],
      "metadata": {
        "id": "JXXB-_f2SyTJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mr1xatZ1SyQT"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}